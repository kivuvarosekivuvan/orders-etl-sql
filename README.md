# Orders ETL + SQL Checks (Mini Project) ğŸ§ªğŸ§©

This repo is a small, working ETL-style pipeline:

- ğŸ§¾ Take raw data from a CSV file (an **orders** dataset)
- ğŸ§½ Clean it (fix formats, remove duplicates, handle missing values)
- ğŸ—ƒï¸ Load it into a relational database
- ğŸ§¿ Run SQL checks to confirm the data is correct
- ğŸ“ˆ Run SQL queries to get simple insights (totals, trends, top items)

This project focuses on solid data habits: clean inputs, clean outputs, and clear checks.

---

## Whatâ€™s inside ğŸ—‚ï¸

- `data/raw/` â†’ raw CSV data (example: `orders.csv`)
- `data/processed/` â†’ cleaned output (optional)
- `sql/`
  - `01_schema.sql` â†’ table setup
  - `02_validation.sql` â†’ data quality checks (duplicates, missing fields, invalid numbers)
  - `03_analytics.sql` â†’ simple insight queries (totals, trends, top products)
- `src/` â†’ pipeline scripts (extract â†’ transform â†’ load)

---

## Pipeline flow (simple) ğŸ§µ

1. **Extract** ğŸ§¾: Read the raw CSV file  
2. **Transform** ğŸ§½: Clean and standardize the data  
3. **Load** ğŸ—ƒï¸: Insert clean data into a relational database  
4. **Validate** ğŸ§¿: Run SQL checks to confirm data quality  
5. **Analyze** ğŸ“ˆ: Run SQL queries to get insights  

---

## How to run ğŸ› ï¸

1) Create and activate a virtual environment
```bash
python3 -m venv .venv
source .venv/bin/activate
```

## 2) Install dependencies ğŸ§°

```bash
pip install -r requirements.txt
```

## 3) Run the pipeline (CSV â†’ clean â†’ database) ğŸ§¾â¡ï¸ğŸ—ƒï¸

```bash
python src/pipeline.py
```

## 4) Run SQL validation + analytics queries ğŸ§¿ğŸ“ˆ

```bash
python src/run_sql.py
```





## Example output âœ…ğŸ§¾

This is a real run result from this repo:

- **Raw rows:** 15  
- **Clean rows:** 12  
- **Validation checks:** all OK (0 bad rows)

Sample analytics results:
- **Total orders:** 12  
- **Total revenue:** 339.50  
- **Top products by revenue:**
  - Wireless Earbuds: 90.00
  - Running Shoes: 60.00
  - Bluetooth Speaker: 55.00
- **Orders by channel:**
  - web: 6
  - mobile: 4
  - partner: 2

What this proves (simple English):
- The pipeline removed or handled bad records (duplicates, missing date, negative amount, missing required fields)
- SQL data checks pass cleanly (0 errors)
- The SQL insights make sense and are useful for business

---

## Data quality checks (SQL) ğŸ§¿

Validation checks for common problems like:

- **Duplicate order IDs** (example: the same `order_id` appears twice)  
- **Missing required fields** (example: missing `order_date` or `amount`)  
- **Invalid numbers** (example: negative `amount` or `quantity = 0`)  
- **Sanity checks** (example: `amount` should match `quantity Ã— unit_price`)  

---

## Analytics queries (SQL) ğŸ“ˆ

The analytics step answers questions like:

- **Total orders and total revenue** (example: â€œHow much did we make?â€)  
- **Revenue trend over time** (example: â€œWhich days/months were strongest?â€)  
- **Top products by revenue** (example: â€œWhich items earn the most?â€)  
- **Revenue by city** (example: â€œWhich city buys the most?â€)  
- **Average order value** (example: â€œWhatâ€™s the usual spend per order?â€)  
- **Orders by channel** (example: â€œWeb vs mobile vs partner â€” what wins?â€)  

---

## Why I built this ğŸ§ 

Iâ€™m building practical skills for data & AI-focused software work:

- Working with structured data  
- SQL validation and analytics  
- Building reliable ETL-style workflows  
- Writing clear, repeatable documentation  

---

## Status ğŸ§·

Working âœ…  
Next updates:
- Add a slightly bigger dataset
- Add screenshots of the outputs in the README
